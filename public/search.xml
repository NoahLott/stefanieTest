<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Chinese PCB Manufacturers should Optimize the Process to Face the coming Challenge</title>
      <link href="/chinese-pcb-manufacturers-should-optimize-the-process-to-face-the-coming-challenge/"/>
      <url>/chinese-pcb-manufacturers-should-optimize-the-process-to-face-the-coming-challenge/</url>
      
        <content type="html"><![CDATA[<h3 id="The-Future-of-Chinese-PCB-Industry"><a href="#The-Future-of-Chinese-PCB-Industry" class="headerlink" title="The Future of Chinese PCB Industry"></a>The Future of Chinese PCB Industry</h3><p>Printed Circuit Board (PCB) is the electrical connection of electronic components. It is used in all but the simplest electronic products and some electrical products. The gross global production value of PCB industry accounts for more than one-quarter of the total output value of the electronic components industry, up to 60 billion US dollars. Due to the huge demand of China domestic market, along with the low labor costs and perfect industrial matching, global PCB production center has been gradually transferred to China since 2000. And in 2006, China has surpassed Japan and become the world’s largest PCB producer.</p><p><img src="/images/9.jpg" alt=""></p><p>On the report of the National Bureau of Statistics, the latest China Manufacturing Purchasing Managers’ Index (PMI) of China as the world’s second-largest economy has hovered between 50.1 and 51.7 index points during 2014, which indicates a positive situation in manufacturing growth. While the PCB industry has a long history in China and has gone through a number of cycles. From the rapid start in 1980-1990 (CAGR = 15.9%), to the sustained growth in 1991-2000 (CAGR = 7.1%), and to the great fluctuations in 2001-2010 (CAGR = 2.1%). Until now it starts a stable growth period in 2011. It is expected that the global PCB will maintain a compound growth rate of 3.2% in 2017-2022. In general, the Chinese manufacturing industry continued to expand at a slower speed and development of the electronic components industry is still on the rise.</p><a id="more"></a><p>According to statistics, the number of professional SMT &amp; PCB factories in China has reached 5000. The total purchases of PCB products, PCBA and SMT services, and the final electronic products in China by overseas purchasers have accounted for more than one-half of their total purchases. However, China still has a long way to go to become a strong PCB power. Previously, industry experts have warned that PCB enterprises should pay attention to the fact that China’s PCB industry will not continue to grow at a high speed in the future. As the result, the product structure of PCB will change dramatically in the future. Not only the quantity will increase, but also the difficulty will rise. </p><p><img src="/images/10.jpg" alt=""></p><p>Therefore, to have better development, PCB companies should keep the pace with the trend of times. In terms of the specific classification, it is expected that by 2021, the proportion of PCB with high technology content, such as multi-layer board, flexible plate, and HDI will reach 60.58%, and become the mainstream of the market. In this case, PCB manufacturers should, on the one hand, follow the latest industry news, on the other hand, optimize the process and continuously improve their automation and intelligence capabilities. In addition, PCB company should also pay attention to the future trends of other electronic industries. The continuous growth of the market will inevitably increase the needs for PCB board and related services. </p><p>One of the best Chinese EMS provider —— Shenzhen Hampoo Science &amp; Technology Co. Ltd has already foreseen the trend. As a <a href="http://www.hampoo.com/" target="_blank" rel="noopener">PCB manufacturing</a> and quick turn solution provider with more than 15 years history, Hampoo has partnered with <strong>Intel, Microsoft, Rockchip, Baidu, TI</strong>, and many other renowned companies from all over the world. In 2017, its high-quality product has won <strong>CET Excellence Partner Award &amp; The Best Digital Transformation Award from Microsoft</strong>. Entitled with “Shenzhen High-tech Enterprise”, Hampoo team has accumulated a wealth experience in hardware design, high-speed PCB signal simulation and design, audio/video technology. Its strong relationship with authorized suppliers, procure parts agents, strict incoming quality control process system and customized solution help the clients to reduce costs and improve the supply efficiency. </p><p><img src="/images/11.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> manufacture </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Top Supercomputer</title>
      <link href="/top-supercomputer/"/>
      <url>/top-supercomputer/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/5.jpg" alt=""></p><h2 id="New-GPU-Accelerated-Supercomputers-Transformation-the-total-amount-of-Power-on-the-TOP500"><a href="#New-GPU-Accelerated-Supercomputers-Transformation-the-total-amount-of-Power-on-the-TOP500" class="headerlink" title="New GPU-Accelerated Supercomputers Transformation the total amount of Power on the TOP500"></a>New GPU-Accelerated Supercomputers Transformation the total amount of Power on the TOP500</h2><p>For the very first time in history, most of the flops added to the TOP500 list originated from GPUs rather than CPUs. Can be this the condition of things to come?</p><p>In the latest TOP500 ranks announced this week, 56 percent of the additional flops were due to NVIDIA Tesla GPUs working in new supercomputers - that based on the Nvidians, who love monitoring such things. In this instance, most of those extra flops originated from three top systems not used to the list: Summit, Sierra, and the AI Bridging Cloud Infrastructure (ABCI). </p><p>Summit, the new Best500 champ, pushed the prior number one system, the 93-petaflop Sunway TaihuLight, into second place with a Linpack score of 122.3 petaflops. Summit is powered by IBM servers, each one built with two Power9 CPUs and six V100 GPUs. Regarding to NVIDIA, 95 percent of the Summit’s peak performance (187.7 petaflops) comes from the system’s 27,686 GPUs.</p><a id="more"></a><p>NVIDIA did an identical calculation for the less powerful, and somewhat less GPU-intense Sierra, which nowadays ranks as the 3rd fastest supercomputer on earth at 71.6 Linpack petaflops. And, although nearly the same as Summit, it features four V100 GPUs in each dual-socked Vitality9 node, rather than six. However, the 17,280 GPUs in Sierra still represent the lion’s share of this system’s flops.</p><p>Likewise for the new ABCI machine found in Japan, which is currently that country’s speediest supercomputer and is ranked fifth on the planet. Each of its servers pairs two Intel Xeon Gold CPUs with four V100 GPUs. Its 4,352 V100s deliver almost all the system’s 19.9 Linpack petaflops.</p><p>As dramatic just as that 56 percent quantity is for latest TOP500 flops, the truth is probably even more impressive.  Relating to Ian Buck, vice president of NVIDIA’s Accelerated Computing business unit, over fifty percent the Tesla GPUs they offer into the HPC/AI/info analytics space happen to be bought by buyers who hardly ever submit their devices for TOP500 consideration. Although many of these GPU-accelerated machines would qualify for an area on the list, these specific customers either don’t value all the TOP500 fanfare or would prefer to certainly not advertise their hardware-buying behaviours to their competitors.</p><p>It’s also value mentioning that the Tensor Cores found in the V100 GPUs, with their specialized 16-bit matrix maths capacity, endow these 3 new systems with more deep learning potential than any previous supercomputer. Summit by itself features over three peak exaflops of deep learning effectiveness. Sierra’s functionality in this respect is more in a nearby of two peak exaflops, while the ABCI number is around half an exaflop. Taken along, these three supercomputers signify extra deep learning capability compared to the other 497 devices on the TOP500 list put together, at least from the perspective of theoretical performance.</p><p>The addition of AI/equipment learning/deep learning into the HPC application space is a comparatively new phenomenon, however the V100 appears to be acting as a catalyst. This year’s TOP500 list represents a distinct shift towards devices that support both HPC and AI computing,� noted TOP500 writer Jack Dongarra, Professor at University of Tennessee and Oak Ridge National Laboratory.</p><p>While company’s like Intel, Google, Fujitsu, Wave Processing, Graph core, and other folks are growing specialized deep learning accelerators for the data center, NVIDIA is sticking with a built-in AI-HPC design because of its Tesla GPU series. And this certainly seems to be paying down, given the growing pattern of using artificial intelligence to accelerate traditional HPC applications.  Although the percentage of users integrating HPC and AI continues to be relatively tiny, this mixed-work flow unit is slowly being extended to practically every technology and engineering domain, from conditions forecasting and economic analytics, to genomics and essential oil &amp; gas exploration.</p><p>Buck admits this interplay around traditional HPC modelling and machine learning continues to be in the initial stages, but maintains it’s only likely to get extra intertwined.� He says despite the fact that some customers use only a subset of the Tesla GPU’s features, the benefits of supporting 64-little bit HPC, equipment learning, and visualization on the same chip far outweighs any positive aspects that could be understood by single-goal accelerators.</p><p>And, thanks in large portion to these deep-learning-enhanced V100 GPUs, mixed-workload equipment are actually popping up about a reasonably regular basis. For instance, although Summit was formerly going to be just another humongous supercomputer, it really is now staying groomed as a program for cutting-edge AI as well. In comparison, the ABCI system was conceived right from the start as an AI-in a position supercomputer that could serve users working both classic simulations and analytics, as well as deep learning workloads. Before this month, the MareNostrum supercomputer added three racks of Power9/V100 nodes, paving just how for critical deep learning job to commence at the Barcelona Supercomputing Center. And even the addition of simply 12 V100 GPUs to the Nimbus cloud assistance at the Pawsey Supercomputing Center was enough to declare that AI would today be fair video game on the Aussie program.</p><p>Seeing that Buck implied, you don’t need to take benefit of the Tensor Cores to really get your money’s well worth from the V100. At seven double-precision teraflops, the V100 is an extremely capable accelerator for regular supercomputing. And regarding to NVIDIA, there are 554 codes ported to these images chips, including all of the leading 15 HPC applications.</p><p>But as V100-powered systems get their way into exploration labs, universities, and professional datacenters, more researchers and engineers will end up being tempted to inject AI into their 64-bit applications. And whether this turns out to be a case of the tail wagging your dog or the various other way around, ultimately, it doesn’t really subject. The HPC application landscape will be forever changed.</p><h2 id="Distortions-Styles-and-Quirks-in-the-June-2018-Leading500-List"><a href="#Distortions-Styles-and-Quirks-in-the-June-2018-Leading500-List" class="headerlink" title="Distortions, Styles and Quirks in the June 2018 Leading500 List"></a>Distortions, Styles and Quirks in the June 2018 Leading500 List</h2><h3 id="Stuffing-the-list"><a href="#Stuffing-the-list" class="headerlink" title="Stuffing the list"></a>Stuffing the list</h3><p>The TOP500 list can be an intensely valuable tool for the HPC community, tracking aggregate trends over 25 years. Nevertheless, a few observers have noted that new publications of the Top rated500 list have many duplicate entries, often at anonymous sites.</p><p>Let’s park the debate on what is a legitimate HPC system or not for the present time and assume that any program that has completed a higher Performance Linpack (HPL) go is a good entry in the list.</p><p>But, there are 124 entries in the list that happen to be identical copies of different entries. Put simply, a single HPL run offers been done on one system, and the vendor has said “Since we’ve marketed N copies of that machine, we can send N entries on the list”.</p><p>What goes on to the list figures if we delete all of the duplicate systems?</p><p>The set of 500 reduces to 376 entries.</p><p>The largest change is Lenovo, dropping from 117 entries to just sixty-one - yes, there are 56 duplicate entries from Lenovo! HPE drops from 79 to 62 entries and retakes the most notable spot for greatest show of the list with 16.5 percent. Lenovo drops to second place with 16.2 percent share.</p><p>Does this matter? Very well, it probably will to Lenovo, who thought we would submit many copies, and HPE, who probably sold various copies but chose not to send the duplicates. And eventually it matters with their market share PR.</p><p>For ordinary people, it comes down to what the list is about. If it likely to list the 500 quickest supercomputers on the globe, clearly it doesn’t do this as many supercomputer owners choose not to acknowledge their devices. Is it the set of regarded supercomputers? No, because several referred to supercomputers are not listed, for instance, Blue Waters. Hence, it can only be a set of acknowledged HPL works, which would recommend that the copies approach is wrong.</p><p>However, it isn’t simply because simple mainly because that. If the list is normally for monitoring vendor/technology market share, then list stuffing is okay - even desired. If the list is usually for monitoring adoption of technology, vendor wins, the fortunes of HPC sites, and improvement of nations, in that case If argue that stuffing in this manner breaks the usefulness of the list.</p><p>The comparison of progress of nations can be affected. Carry out we measure just how many devices deployed? Or who gets the biggest system? I think the list stuffing is certainly less critical below, as we are able to readily extract both developments.</p><p>I’m not sure there is a right or wrong answer to this but, as often, the headline statistics of market share just tell one aspect of the story, and users of the list must dig deeper for appropriate insight.</p><h3 id="Anonymity-rules"><a href="#Anonymity-rules" class="headerlink" title="Anonymity rules"></a>Anonymity rules</h3><p>Another value of the list over time has been the capability to track who is ordering supercomputers and what they are employing them for.</p><p>In the June 2018 list, 97 percent of the systems do not specify an application area. This implies this categorization is actually meaningless. Either drop it from upcoming lists or require long term systems to identify an application area.</p><p>But, beyond that, the June 2018 List features 283 (!) anonymous entries. That’s over one half of the list where we have no idea the business or site that has deployed the system nor, in most cases, what it really is being used for.</p><p>The big question is: does that render meaningless the opportunity to track the who and what of HPC, or would we be in a worse position if we excluded anonymous systems?</p><p>There are maybe 238 systems which can be lumped into cloud / IT service / web hosting companies, and it is the sheer quantity of these that delivers the possibly unhelpful distortion.</p><p>The rest of the 45 arguably represent real HPC deployments and have plenty of categorization (e.g., “Strength Company” to be useful. Useful guesses can even be produced for some of these anonymous systems. For example, one might reckon that “Energy Firm (A)” found in Italy is in fact Eni. The 45 devices are a mixture of energy, manufacturing, authorities, or finance sectors. Many interestingly, a few university devices are listed anonymously also!</p><h3 id="Supercomputers-in-Industry"><a href="#Supercomputers-in-Industry" class="headerlink" title="Supercomputers in Industry"></a>Supercomputers in Industry</h3><p>Of the 284 devices listed as Industry, only 16 systems actually have named owners that aren’t cloud suppliers (the other 268 are usually the anonymous companies and Stuffing discussed above). In fact, due to multiple systems per web page, we actually only employ a few listed companies. These are Eni, Total, PGS, Saudi Aramco, BASF, EDF, Volvo and the suppliers Intel, NVIDIA, and PEZY.</p><p>I just assure you there are a lot more supercomputers out there in industry than this. I intend to explore the Major500 tendencies, along lessons from my impartial HPC consulting job, of HPC systems in industry for a future article. Look out for this after ISC18.</p><h3 id="Hope"><a href="#Hope" class="headerlink" title="Hope"></a>Hope</h3><p>I’ve previously said that the HPC is lucky to really have the TOP500 list. It is a info set collected in a regular manner twice annually for 25 years, with each entry recording many characteristics - vendor, technology details, effectiveness, location, year of entry, etc. That is a hugely wealthy resource for our network, and the TOP500 authors did an amazing task over 25 years to keep carefully the list valuable as the world of HPC has evolved enormously. I have high confidence in the authors efficiently addressing the challenges right here and keeping the list as a great resource for years to come.</p>]]></content>
      
      
      <categories>
          
          <category> recommended of the year </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Upgrade The Supercomputer</title>
      <link href="/upgrade-the-supercomputer/"/>
      <url>/upgrade-the-supercomputer/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/6.jpg" alt=""></p><p>The TOP500 table shows the 500 most powerful commercially available computer systems known to us. To keep the list as compact as feasible, we show only part of our information here:</p><ul><li><p>Nworld - Location within the TOP500 ranking</p></li><li><p>Manufacturer - Manufacturer or vendor</p></li><li><p>Computer system - Type indicated by maker or vendor</p></li><li><p>Installation Site - Customer</p></li><li><p>Location - Location and country</p></li><li><p>Year - Year of assembly/last major update</p></li><li><p>Field of Application</p></li><li><p>#Proc. - Quantity of processors (Cores)</p></li><li><p>Rmax - Maximal LINPACK performance achieved</p></li><li><p>Rpeak - Theoretical peak performance</p></li><li><p>Nmax - Issue size for achieving Rmax</p></li><li><p>N1/2 - Problem size for attaining half of Rmax</p></li></ul><p>If Rmax from Desk 3 of the LINPACK Report is not available, we use the TPP performance provided in Table 1 of the LINPACK Report for solving something of 1000 equations. In a few circumstances we interpolated between two measured program sizes or we scaled by cycle times. For products where we did not receive the requested data, the performance of another smaller system measured can be used.</p><a id="more"></a><p>If there must be any improvements in the performances given in the next desk we will update them.</p><p>In addition to cross checking several resources of information, we select randomly a statistical representative sample of the first 500 devices of our database. For these devices we ask the provider of the information to establish direct contact between your installation blog and us to verify the granted information. Thus giving us basic info on the quality of the list in total.</p><p>As the TOP500 should give a basis for figures that you can buy of high-performance PCs, we limit the quantity of devices installed at vendor sites. That is done for each and every vendor individually by limiting the accumulated effectiveness of systems at vendor sites to a maximum of 5% of the total accumulated installed effectiveness of the vendor. Rounding is performed in favour of the vendor in question.</p><p>In the TOP500 List table, the computers are ordered initially by their Rmax value. In the case of equal performances (Rmax benefit) for different computers, we’ve chosen to buy by Rpeak. For sites that contain the same pc, the purchase is by recollection size and alphabetically.</p><h2 id="DEMAND-PARTICIPATION-IN-THE-Top-rated500-GREEN500-LISTS"><a href="#DEMAND-PARTICIPATION-IN-THE-Top-rated500-GREEN500-LISTS" class="headerlink" title="DEMAND PARTICIPATION IN THE Top rated500 / GREEN500 LISTS"></a>DEMAND PARTICIPATION IN THE Top rated500 / GREEN500 LISTS</h2><p>The TOP500 project was started in 1993 to provide a reliable basis for tracking and detecting trends in high-performance computing. Twice a yr, a list of the sites operating the 500 most effective computer systems is assembled and introduced. The best efficiency on the Linpack benchmark is utilized as efficiency measure for ranking the personal computers. The list consists of a range of information like the system specs and its major application areas.</p><p>Each year the 1st TOP500 list is released through the ISC High Performance Meeting. In 2018, the conference will be held in Frankfurt, Germany (June 24 - 28) and the SC conference in America, this year placed in Dallas (November 11th-16th, 2018)</p><p>The List’s maintainers are enthusiastic about new entries and entries that are no more valid. Please feel absolve to contact them in this article should you have questions.</p><h3 id="Following-is-a-transcript-of-the-guidelines-for-submission-of-entries"><a href="#Following-is-a-transcript-of-the-guidelines-for-submission-of-entries" class="headerlink" title="Following is a transcript of the guidelines for submission of entries:"></a>Following is a transcript of the guidelines for submission of entries:</h3><p><em>Deadlines</em><br>The deadline for the submission of new sites for the November release of the TOP500 List is October 25th (23:59 PST). All system reported need to be set up by November 1st. The last time for submitting Linpack updates can be November 1st ( Systems will need to have been submitted by the deadline above).</p><p>Submission deadline for the June release of the TOP500 List is June 1st (23:59 PST). All system reported must be installed by June 8th. The last date for submitting Linpack improvements can be June 8th. (Devices will need to have been submitted by June 1st)</p><p><em>Submissions</em><br>The submission site is now open. You may want to make a new account in order to submit systems.</p><p>Please note that this is a totally new interface and you possess to produce a new login and password.</p><p>Submissions should support the following information:</p><p>Manufacturer: Manufacturer or vendor</p><p>Computer program: Type indicated by company or vendor</p><p>Installation Site: Customer - Name of company or institution</p><p>Location: Location and country</p><p>Year: Year of assembly/last major update</p><p>Field of Application: Kind of customer (university, federal government, industry, …) in addition to typical application (geophysics, motor vehicle, chemical, benchmarking, …)</p><p>Processors: Quantity of processors (and also type if necessary)</p><p>Memory: Main memory construction of the system</p><p>By submitting the application to be contained in the List TOP500 published on www.top500.org you agree to the following conditions and conditions:</p><p>To the very best of your understanding, the info you provide to TOP500 and its own operator Prometeus GmbH is correct, complete and accurate.<br>You acknowledge and concur that your choice of TOP500 and its operator Prometeus GmbH to add or not include your brand in the list of TOP500 is entirely predicated on Prometeus� discretion rather than at the mercy of judicial review. This as well applies to the position the TOP500 and its operator Prometeus GmbH assigns to you on the set of TOP500.<br>The TOP500 and its own operator Prometeus GmbH may, with or without prior notice and with or without reason strike your name from the TOP500 List. This technique and decision is not at the mercy of judicial review.<br>EEHPC WG: Electric power Measurement Methodology<br>Click the link below to download the EEHPC WG Electric power Measurement Methodology to determine more about Level 2 and Level 3 measurements.</p><p><em>Linpack performance</em><br>Performance figures for the Linpack benchmark are collected and updated by Jack Dongarra (dongarra@cs.utk.edu). Please article any Linpack performance quantities directly to him.</p><p><em>Method of Solution</em><br>In an attempt to get hold of uniformity across all computers in performance reporting, the algorithm used in solving the system of equations in the benchmark process must confirm to LU factorization with partial pivoting. Specifically, the operation count for the algorithm should be 2/3 n^3 + O(n^2) double stage floating point operations. This excludes the consumption of a fast matrix multiply algorithm like ``Strassen’s Method’’ or algorithms which compute a remedy in a precision lower than full precision (64 bit floating level arithmetic) and refine the perfect solution is using an iterative strategy. This is done to supply a comparable set of performance quantities across all PCs.</p><p><em>Verification</em><br>Furthermore to cross checking several sources of information, we select randomly a statistical representative sample of the 1st 500 devices of our data source. For these systems we ask the distributor of the information to establish direct contact between the installation blog and us to verify the offered information. Thus giving us basic information about the quality of the list altogether.</p><p><em>Restrictions</em><br>As the TOP500 should provide a basis for figures available of high-performance computer systems, we limit the quantity of systems installed at vendor sites. This is done for every single vendor individually by limiting the accumulated efficiency of devices at vendor sites to no more than 5% of the total accumulated installed efficiency of the vendor. Rounding is performed in favor of owner in question.</p><p>The main objective of the TOP500 list is to supply a ranked list of general purpose systems that are in common use for top quality applications. The authors of the Leading500 reserve the right to individually verify submitted Linpack effects, and exclude systems from the list that are not valid or not general goal in nature. By general purpose system we imply that the computer system must be in a position to be used to fix a range of scientific challenges. Any system designed especially to fix the Linpack benchmark trouble or possess as its important purpose the purpose of a high Top rated500 ranking will get disqualified.</p><p>The devices in the TOP500 list are anticipated to come to be persistent and available for use for a protracted period of time. Any program assembled to run a Linpack benchmark simply, and create specifically to get an access in the TOP500 will come to be excluded from the list. The Major500 authors will reserve the right to deny inclusion in the list if it is suspected that the machine violates these conditions.</p>]]></content>
      
      
      <categories>
          
          <category> supercomputer </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>The Linpack Benchmark</title>
      <link href="/the-linpack-benchmark/"/>
      <url>/the-linpack-benchmark/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/7.jpg" alt=""></p><p>As a yardstick of efficiency we are employing the `best’ effectiveness as measured by the LINPACK Benchmark. LINPACK was picked because it is trusted and performance quantities are available for virtually all relevant systems.</p><p>The LINPACK Benchmark was introduced by Jack Dongarra. An in depth description in addition to a list of performance outcomes on a wide variety of machines comes in postscript contact form from net-lib. Below you can download the most recent release of the LINPACK Article: efficiency.ps. A parallel execution of the Linpack benchmark and instructions how to run it.</p><p>The benchmark found in the LINPACK Benchmark is to solve a dense system of linear equations. For the Top rated500, we used that type of the benchmark which allows an individual to scale how big is the problem also to optimize the software to be able to achieve the very best performance for confirmed machine. This performance does not reflect the overall efficiency of confirmed system, as no single number ever before can. It does, however, reflect the functionality of a committed program for solving a dense system of linear equations. Since the situation is quite regular, the overall performance achieved is fairly high, and the effectiveness numbers give a good correction of peak overall performance.</p><a id="more"></a><p>By measuring some of the performance for different issue sizes n, a consumer can get not merely the maximal achieved effectiveness Rmax for the problem size Nmax but also the condition size N1/2 where half of the overall performance Rmax is achieved. These numbers alongside the theoretical peak functionality Rpeak are the figures given in the Leading500. In an attempt to obtain uniformity across all personal computers in effectiveness reporting, the algorithm used in solving the system of equations in the benchmark procedure must comply with LU factorization with partial pivoting. In particular, the procedure count for the algorithm should be 2/3 n^3 + O(n^2) dual precision floating point operations. This excludes the use of an easy matrix multiply algorithm like “Strassen’s Technique” or algorithms which compute a solution in a precision lower than full precision (64 bit floating point arithmetic) and refine the answer using an iterative strategy.</p><p>As a yardstick of effectiveness we are employing the `best’ performance as measured by the LINPACK Benchmark. LINPACK was picked because it is trusted and performance quantities are available for virtually all relevant systems.</p><p>The LINPACK Benchmark was introduced by Jack Dongarra. A detailed description in addition to a list of performance results on a wide variety of machines comes in postscript web form from net-lib.</p><p>The benchmark found in the LINPACK Benchmark is to resolve a dense system of linear equations. For the Major500, we applied that edition of the benchmark which allows an individual to scale how big is the problem and to optimize the software so that you can achieve the very best performance for confirmed machine. This performance will not reflect the overall efficiency of confirmed system, as no number ever can. It does, however, reflect the overall performance of a devoted program for solving a dense program of linear equations. Since the situation is very regular, the functionality achieved is quite high, and the functionality numbers give a good correction of peak efficiency.</p><p>By measuring the actual performance for different difficulty sizes n, a end user can get not merely the maximal achieved overall performance Rmax for the situation size Nmax but also the situation size N1/2 where half of the performance Rmax is achieved. These numbers together with the theoretical peak effectiveness Rpeak are the numbers given in the Top rated500. In an attempt to get hold of uniformity across all pcs in effectiveness reporting, the algorithm found in solving the machine of equations in the benchmark treatment must conform to LU factorization with partial pivoting. Specifically, the operation count for the algorithm must be 2/3 n^3 + O(n^2) dual precision floating point functions. This excludes the utilization of an easy matrix multiply algorithm like “Strassen’s Approach” or algorithms which compute a remedy in a precision lower than full precision (64 bit floating stage arithmetic) and refine the answer using an iterative way.</p><h2 id="The-Authors"><a href="#The-Authors" class="headerlink" title="The Authors"></a>The Authors</h2><h3 id="HANS-WERNER-MEUER-1936-2014"><a href="#HANS-WERNER-MEUER-1936-2014" class="headerlink" title="HANS WERNER MEUER (1936 - 2014)"></a>HANS WERNER MEUER (1936 - 2014)</h3><p>In 1993, Hans Meuer started the TOP500 project together with Erich Strohmaier (previously at the University of Mannheim, currently at Lawrence Berkeley National Laboratory) and Jack Dongarra (University of Tennessee and ORNL). From 1976 until his retirement in 2000, Meuer was director of the processing center and professor for computer technology at the University of Mannheim, Germany. While at the university, Meuer was co-founder and organizer of the first of all Mannheim Supercomputer Seminar in 1986, an gross annual appointment referred to today as the International Supercomputing Meeting (ISC).Meuer features served as the Meeting General Chair since the very beginning. Since 1998, he has been managing director of Prometeus GmbH.</p><p>The TOP500 list was created as a project for the June 1993 meeting in Mannheim, and updated for the Supercomputing 93 conference held that November. The list continues to be released at ISC every June and SC in November.</p><p>Ahead of joining the University of Mannheim, Meuer served as specialist, task leader, group and department chief during his 11 years at the study Center in Julich,Germany, from 1962 - 73. Furthermore to his afore stated activities, Hans Meuer offers been editor-in-chief of the professional IT journal PIK - Praxis der Informationsverarbeitung und Kommunikation(published by KG Saur VerlagMunchen, Munich), from 1986 - 2004. He was a member of the professional societies ACM and GI. He posted numerous articles in the areas of mathematics, data processing and computer research.</p><p>Meuer studied mathematics, physics and politics in the universities of Marburg, Giessen and Vienna. He graduated at the University of Giessen in 1962and in 1972, he received his doctorate in used mathematics from the Rheinisch Westfalischen Complex University (RWTH) of Aachen. </p><p>Prof. Dr. Hans Werner Meuer passed on at the age of 77 at his residence in Daisbach, Southern Germany, on January 20, 2014, after a short battle with cancer.</p><h3 id="ERICH-STROHMAIER’"><a href="#ERICH-STROHMAIER’" class="headerlink" title="ERICH STROHMAIER’"></a>ERICH STROHMAIER’</h3><p>Dealing with Prof. Hans Meuer, Erich Strohmaier created the primary Major500 list in June 1993. On his primary day at the brand new task at the University of Mannheim, he attended a tiny university conference, the “Mannheimer Supercomputer Seminar” organized by Meuer and Dr. Hans-Martin Wacker. One of is own duties while working for Prof. Meuer was to put together statistics on all over the world supercomputers in planning for the annual assembly of the meeting. Thinking it could be a one-time deal, Strohmaier created a data source on his pc for just that. But then Meuer and Strohmaier made a decision to see how much the list would switch in five a few months and recalculated the list in time to provide the effects at the 1993 Supercomputing meeting placed in November in Portland, Oregon. The Leading500 list of the world’s leading supercomputers was born.</p><p>Strohmaier earned his Ph.D. in theoretical physics in 1990. Since his thesis centered on numerical strategies in elementary particle physics that he used the largest supercomputers obtainable, he accepted a study position in HPC at the University of Mannheim for a fresh project comparing the effectiveness of a number of physics applications on a Fujitsu VP2600 supercomputer. In 1995, as that exploration funding was closing, he decided to choose a position in the U.S. and ended up dealing with Linpack writer and fellow TOP500 editor Jack Dongarra, at the University of Tennessee. He relocated to Lawrence Berkeley National Laboratory in 2001. Furthermore to his current role as head into the future Technology Group at Berkeley Lab, he is also the principal investigator of the Department of Energy-funded CACHE Institute, a joint mathematics and computer system science institute centered on Interaction Avoiding and Interaction Hiding at Intensive Scales. In 2008, he was an associate of the staff which received the ACM Gordon Bell Prize for Algorithm Development.</p><h3 id="JACK-DONGARRA"><a href="#JACK-DONGARRA" class="headerlink" title="JACK DONGARRA"></a>JACK DONGARRA</h3><p>Jack Dongarra has been involved because the origin and formation of the Major500 list in 1993, that used his Linpack benchmark seeing that the common request for evaluating the overall performance of supercomputers. Through the constant usage of the Linpack benchmark, the Major500 list offers a standardized measure of supercomputers over the past 25 years. Dongarra retains appointments at the University of Tennessee and Oak Ridge National Laboratory. He’s a Faculty Fellow at Texas A&amp;M University’s Institute for Advanced Analysis and Turing Fellow at the University of Manchester. He’s also an Adjunct Professor at Rice University. He specializes in numerical algorithms in linear algebra, parallel computing, use of advanced-computer system architectures, programming methodology, and tools for parallel personal computers. </p><p>Furthermore to Linpack and the TOP500, Dongarra has contributed to the look and implementation of the following open source software packages and devices: EISPACK, the BLAS, LAPACK, ScaLAPACK, Net-lib, PVM, MPI, Open-MPI, NetSolve, ATLAS, PAPI, PLASMA, and MAGMA. He has published approximately 200 articles, papers, studies and specialized memoranda and he’s co-author of several literature. He was awarded the IEEE Sid Fernbach Award in 2004 for his contributions in the use of powerful computers using innovative techniques; in 2008 he was the recipient of the primary IEEE Medal of Excellence in Scalable Computing; in 2010 2010 he was the earliest recipient of the SIAM Particular Fascination Group on Supercomputer’s award for Job Achievement; and in 2011 he was the recipient of the IEEE IPDPS 2011 Charles Babbage Award. He is a Fellow of the AAAS, ACM, IEEE, and SIAM and a member of the National Academy of Engineering. Dongarra provides been an ISC Fellow since 2012.</p><p>Dongarra received a Bachelor of Science found in Mathematics from Chicago Condition University found in 1972 and a Expert of Science in Pc Technology from the Illinois Institute of Technology found in 1973. He received his Ph.D. in Applied Mathematics from the University of New Mexico in 1980. He worked at the Argonne National Laboratory until 1989, learning to be a senior scientist. He’s the director of the Impressive Processing Laboratory at the University of Tennessee.</p>]]></content>
      
      
      <categories>
          
          <category> news </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>News Team</title>
      <link href="/news-team/"/>
      <url>/news-team/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/8.jpg" alt="Ai"></p><h2 id="EDITORIAL"><a href="#EDITORIAL" class="headerlink" title="EDITORIAL"></a>EDITORIAL</h2><h3 id="Michael-Feldman-Managing-Editor"><a href="#Michael-Feldman-Managing-Editor" class="headerlink" title="Michael Feldman, Managing Editor"></a>Michael Feldman, Managing Editor</h3><p>Michael Feldman has covered the powerful computing industry for 10 years, first while the managing editor of HPCwire, then as market research analyst in Intersect360 Research. Throughout that period, he founded himself as trusted tone of voice in the HPC network. Prior to his are a journalist and industry analyst, Michael was a application engineer, a profession that spanned 25 years, and included tasks in compiler/runtime platforms, software development equipment, and real-time embedded systems.</p><h2 id="PUBLISHING"><a href="#PUBLISHING" class="headerlink" title="PUBLISHING"></a>PUBLISHING</h2><h3 id="Martin-Meuer-Publisher"><a href="#Martin-Meuer-Publisher" class="headerlink" title="Martin Meuer, Publisher"></a>Martin Meuer, Publisher</h3><p>Martin Meuer may be the ISC Conference Standard Co-Chair and Managing Director of the ISC Group. Since 2002, he has been a great drive in growing the ISC brand beyond Europe, and elevating the ISC exhibition to an international stature. Prior to becoming a member of ISC Group, Martin kept numerous positions in the IT secureness sector. He holds a graduate level in Computer Research from the University of Karlsruhe, Germany.</p><h3 id="Thomas-Meuer-Publisher"><a href="#Thomas-Meuer-Publisher" class="headerlink" title="Thomas Meuer, Publisher"></a>Thomas Meuer, Publisher</h3><p>Thomas Meuer may be the ISC Conference Standard Co-Chair and Managing Director of ISC Group since 2014. From 2007 - 2014, he was the company’s economic director. He keeps a graduate degree running a business Administration from the University of Mannheim and is a Certified Information Devices Auditor (CISA). Ahead of signing up for ISC Group, Thomas put in over a decade in IT Protection, Consulting, Finance and Risk Supervision in a variety of multinational companies, serving last but not least as an IT Audit Manager at KPMG in Berlin.</p><a id="more"></a><h2 id="SALES-amp-MARKETING"><a href="#SALES-amp-MARKETING" class="headerlink" title="SALES &amp; MARKETING"></a>SALES &amp; MARKETING</h2><h3 id="Anna-Clarke-Brand-amp-Organization-Development-Manager"><a href="#Anna-Clarke-Brand-amp-Organization-Development-Manager" class="headerlink" title="Anna Clarke, Brand &amp; Organization Development Manager"></a>Anna Clarke, Brand &amp; Organization Development Manager</h3><p>Anna Clarke is accountable for the branding of all divisions of ISC Group, including ISC POWERFUL, Best500 and ZetaLeads. She acts as a job manager for the Major500 business device by coordinating work to develop the Best500 website and its own news offering. Before becoming a member of ISC Group, Anna held several advertising positions in the telecommunications and hospitality sectors. She retains an M.A good. degree in Communication Studies and Media Research, Organization Administration and Inter cultural Communications from the LMU Munich. </p><h3 id="Darren-Ebbs-Sales-Manager"><a href="#Darren-Ebbs-Sales-Manager" class="headerlink" title="Darren Ebbs, Sales Manager"></a>Darren Ebbs, Sales Manager</h3><p>Darren Ebbs is the Sales Manager working on the TOP500 project and linked publications. He manages all sales activities linked to the TOP500 and Green500 makes and supports advertisers to find the most suitable opportunities to achieve their marketing goals. Prior to doing work for ISC Group, Darren offers been working in POWERFUL Computing media going back a decade and brings an abundance of knowledge in the marketplace.</p><h3 id="Nages-Sieslack-MARKETING-AND-SALES-COMMUNICATIONS-Manager"><a href="#Nages-Sieslack-MARKETING-AND-SALES-COMMUNICATIONS-Manager" class="headerlink" title="Nages Sieslack, MARKETING AND SALES COMMUNICATIONS Manager"></a>Nages Sieslack, MARKETING AND SALES COMMUNICATIONS Manager</h3><p>Nages Sieslack manages the marketing and sales communications of most ISC Group businesses, including the ISC POWERFUL conference and TOP500. In her part, she promotes the ISC meeting and TOP500 project to all or any stakeholders via a number of communication channels, like the press and cultural media. She also develops sponsorship courses with publications and additional HPC-related conferences. Portion of her obligations includes building romantic relationships with HPC community customers. Nages speaks four languages and is usually adept in cross-cultural interaction. She keeps a bachelor’s degree in Laws and an M.S. in Marketing.  </p><h2 id="GUEST-CONTRIBUTORS"><a href="#GUEST-CONTRIBUTORS" class="headerlink" title="GUEST CONTRIBUTORS"></a>GUEST CONTRIBUTORS</h2><h3 id="Chris-Downing-Crimson-Oak-Consulting"><a href="#Chris-Downing-Crimson-Oak-Consulting" class="headerlink" title="Chris Downing, Crimson Oak Consulting"></a>Chris Downing, Crimson Oak Consulting</h3><p>Chris Downing joined Red Oak Consulting in 2014 on completion of his PHD thesis in computational chemistry at University University London. Having performed academic research using the previous two UK countrywide supercomputing solutions (HECToR and ARCHER) as well as a number of small HPC resources, Chris knows the complexities of complementing both hardware and software to consumer requirements. His detailed understanding of products chemistry and solid-talk about physics means that he is well-placed to provide insight into emerging technology. Chris, Senior Consultant, includes a highly technical skill set working mainly in the technology and research staff providing a broad selection of technical consultancy services. </p><h3 id="Andrew-Jones-Vice-President-HPC-Organization-NAG"><a href="#Andrew-Jones-Vice-President-HPC-Organization-NAG" class="headerlink" title="Andrew Jones, Vice-President HPC Organization, NAG"></a>Andrew Jones, Vice-President HPC Organization, NAG</h3><p>Andrew is business lead for strategic HPC companies and consulting in NAG. NAG operates a HPC center of excellence serving UK, USA and additional countries. Andrew offers over twenty years of knowledge in HPC, supercomputing and scientific processing in government, sector and academia. He possesses been an end-individual of HPC technology in study, a software developer, a HPC provider manager, involved with various acquisition and strategy projects. Andrew can be an active member of the international HPC network. He are available on twitter as @hpcnotes.</p><h3 id="James-Reinders-Parallel-Development-Expert"><a href="#James-Reinders-Parallel-Development-Expert" class="headerlink" title="James Reinders, Parallel Development Expert"></a>James Reinders, Parallel Development Expert</h3><p>James has over 30 years of experience seeing that a software engineer found in high performance processing (HPC) and parallel computing, including 27 years while at Intel Company. As a senior engineer at Intel, he contributed to several high-profile projects, including the world’s primary teraflops supercomputer (ASCI Crimson) and the world’s 1st teraflops microprocessor (Intel Xeon Phi). He’s the writer of eight books, in addition to numerous papers and sites, in the HPC discipline.</p><h3 id="Addison-Snell-Intersect360-Research"><a href="#Addison-Snell-Intersect360-Research" class="headerlink" title="Addison Snell, Intersect360 Research"></a>Addison Snell, Intersect360 Research</h3><p>Together with Michael Feldman, Addison Snell co-hosts the pod cast “This Week found in HPC”, published weekly in TOP500 News.  He’s the CEO of Intersect360 Study and a veteran of the POWERFUL Computing industry. He launched the business in 2007 just as Tabor Analysis, a division of Tabor Communications, and served just as that company’s VP/GM until he and his spouse, Christopher Willard, Ph.D., acquired Tabor Exploration in 2009 2009. During his tenure, Addison has generated Intersect360 Exploration as a premier way to obtain market information, research, and consulting.</p>]]></content>
      
      
      <categories>
          
          <category> news </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>How To Program Supercomputer</title>
      <link href="/how-to-program-supercomputer/"/>
      <url>/how-to-program-supercomputer/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/3.jpg" alt=""></p><p>We felt that it had been essential to write a reserve reintroducing programming tactics that should be employed by software developers targeting the existing and future generation supercomputers. As the techniques have been around for a long time, many of today’s developers are not aware of them. Let us explain.</p><p>The supercomputer is a shifting target for application programmers since its inception in the form of Seymour Cray’s CDC 6600 in the first 1970s, forcing developers to adjust to new approaches together with the ever-changing hardware and software systems. This need of programmer adaptation is especially conspicuous in neuro-scientific high-effectiveness processing (HPC), where developers typically optimize for the prospective node architecture to squeeze out every previous ounce of available effectiveness. This continued through the CDC7600 and the vector-rich systems: Cray 1, Cray XMP, YMP, and C90.</p><p>Then, the “Attack of the killer micros” in the 1990s threw everything right into a tizzy. With raising node counts, request developers possessed to consider PVM, and MPI, within their quest to parallelize their applications across the multitude of nodes comprising commodity off-the-shelf (COTS) chips without the vector instructions. As can be seen in Figure 1, the COTS chips acquired more rapidly as their clock routine time decreased between 1995 and 2010. Software developers no more had to be worried about the node architecture.</p><a id="more"></a><p>With the advent of AVX256, AVX512, SVE, and GPUs in the last five years, vectors have begun to come back. More recently, many-core devices like Intel’s Knight’s Landing (KNL) in addition to fastened accelerators such as for example Nvidia’s type of GPUs required a re-examination of the application form to progress performance from the brand new, more-powerful nodes. Because the application could not really always be vectorized and parallelized instantly by the compiler, the application form developer had to do something.</p><p>That something ranged from writing important kernels in particular programming models like CUDA for the Nvidia GPUs, to using compiler directives to greatly help the compiler translate the input user-level code into low-level vectorized code for the processors. In that case there was the issue of using those cores on the node / making a large number of threads for the GPU. Because jogging MPI across all the cores on and off the nodes proved helpful pretty much, the developers for the original multi core systems didn’t should get into parallelizing with shared recollection threads. Even so, threading on Nvidia accelerators was definitely required. OpenACC and OpenMP 4.5 were developed for a performance-portable solution for threading and vectorization for the GPU.</p><p>Another revelation in different architectures is that memory space hierarchies have become more complex, especially if an application has a large memory space footprint. Knight’s Landing, together with nodes with fastened GPU accelerators, possess two levels of recollection which introduces new issues that must be tackled by programmers. Shape 2 shows the dissimilarities between the recollection hierarchies of KNL and the hosted GPU. If an application fits in to the high-speed memory, then it’ll enjoy excellent memory performance. Nevertheless, if the application form requires more memory, the info set must be managed to flow between the two within an efficient and timely approach.</p><p>The last four to five years have already been a culture shock to those creators who insisted on productivity over performance. They are actually confronted with the serious task of properly utilizing these new strong nodes. If indeed they continue with all-MPI codes and do not perform the required conversion to multi-level parallel code (through vectorization and threading), their performance will be a function of merely the amount of cores on the node and the clock cycle. While the amount of cores on the node is certainly going up gradually, the clock rate of those cores is going down, leading to poor returns on latest hardware investments without software optimization.</p><p>Our book Development for Hybrid Multi / Manycore MPP Devices discusses the architecture of the brand new nodes and programming techniques application developers may utilize to glean more performance. It’ll be extremely useful for all those developers who are now confronted with the significant challenge of getting increased effectiveness from the vector capacity for the node together with improved scaling over the increased amount of cores or threading for the GPU. The reserve also looks at the memory space hierarchy of the KNL and discusses many approaches for managing the data. Finally, the reserve looks to the near future, which is extra of the same in lots of ways: extra cores with wider vectors and also more technical memory hierarchies.</p><p>If you’ll be at SC17 in Denver, be a part of us at Cray booth #625 for a good meet-the-author session at 2 p.m. on Tuesday, November 14. You can purchase a reserve at the CRC Press booth #811 for a 30% discount through the show.</p><h2 id="Your-Own-Supercomputer-in-the-Azure-Cloud"><a href="#Your-Own-Supercomputer-in-the-Azure-Cloud" class="headerlink" title="Your Own Supercomputer in the Azure Cloud"></a>Your Own Supercomputer in the Azure Cloud</h2><p>Your organization depends on technology for a competitive advantage. You will need fast, reliable effectiveness. But high-performance computing can be a large expense, and challenging to manage in your IT infrastructure.</p><p>Cray and Microsoft produce it easy - and you get Cray supercomputing found in the Microsoft Azure cloud.<br>This convergence of supercomputing capabilities and the ease of cloud management enables disruptive innovation for new types of companies. It alleviates the operations burden and reduces barriers of access to HPC.</p><p>Supercomputing is no more out of grab smaller enterprises and corporations that can’t or don’t want to maintain their own datacenters.</p><p>Intersect360 analyst Addison Snell says, the mixture of Cray’s systems and support capability, combined with access to Azure’s robust services, can go well beyond traditional IT resources and unlock possibilities that have been previously impossible because of scaling and performance limitations.</p><h3 id="Topics-include"><a href="#Topics-include" class="headerlink" title="Topics include:"></a>Topics include:</h3><p><em>Market dynamics</em> - vertical industry distribution of HPC usage<br><em>Cray found in Azure</em> - differentiated capabilities, managing Cray in Azure, and expanding high-performance workloads<br><em>Research analysis</em> - searching toward AI, big data and HPC<br>He as well examines two industry tendencies: (1) big data and analytics, which inspire businesses to unlock the competitive positive aspects hiding unseen within their data; and (2) artificial cleverness (AI), fuelled by developments in equipment learning algorithms created by leading hyper scale processing companies such as for example Microsoft, Google, Facebook, and others.</p><p>Just as big info swept through the IT landscape, many companies are actually launching investigations of how AI might help them automate techniques, improve responsiveness, and increase competitiveness.</p><p>Download the full paper, “Azure Provides Clean On-Ramp to Cray Processing”, to find out more on today’s HPC and cloud landscape, and to find out how Cray in Azure could work for your enterprise.</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>China And Japan Supercomputer</title>
      <link href="/china-and-japan-supercomputer/"/>
      <url>/china-and-japan-supercomputer/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/2.jpg" alt=""></p><h2 id="Supercomputer-slowdown-World’s-quickest-system-sees-zero-new-challengers"><a href="#Supercomputer-slowdown-World’s-quickest-system-sees-zero-new-challengers" class="headerlink" title="Supercomputer slowdown: World’s quickest system sees zero new challengers"></a>Supercomputer slowdown: World’s quickest system sees zero new challengers</h2><p>HPC growth stalls with only 1 new supercomputer cracking top list.</p><p>The group that measures the world’s Top 500 most effective supercomputers hasn’t crowned a new champion in more than a year.</p><p>Tianhe-2, of China’s National Super Computer Center, overran the top spot in June 2013 with a measured speed of 33.86 petaflop/s, and it held on to #1 in both November 2013 list and the June 2014 list released yesterday.</p><p>The follow-up to Tianhe-1A, Tianhe-2 uses Ivy Bridge-based Intel Xeons and Intel Xeon Phi for a total of 3.12 million cores. The computer uses 17,808 kilowatts of ability for 1.9 gigaflop/s per watt and will theoretically hit speeds of up to 54.9 petaflops.</p><a id="more"></a><p>A system maintaining the #1 place three times found in a row isn’t unprecedented: IBM’s US-based Roadrunner, the first petaflop equipment, won three right titles from June 2008 to June 2009. But in the hottest list, the very best nine machines are similar to those from half a year ago. And even underneath of the most notable 500 is seeing significantly less turnover and growth than usual.</p><p>“Since its inception in June 1993, the TOP500 list has offered as a consistent measure of the performance growth of supercomputers, since all systems are rated according to performance running the same Linpack benchmark request,” yesterday’s announcement explained. “For the next consecutive list, the entire growth fee of all systems is at a historical low.”</p><p>The just new entry in the top 10 “was at number 10-a 3.14 petaflop/s Cray XC30 installed at an undisclosed US federal government site,” the very best 500 job leaders wrote. A petaflop is definitely one quadrillion, or a thousand trillion, calculations per second.</p><p>“Things seem to be slowing down”, University of Tennessee professor Jack Dongarra, who created the Linpack benchmarks and helps compile the bi-annual Leading 500 list, told Wired. You may characterize it as maybe a sign that Moore’s Laws is having some concerns.</p><p>Some further stats help illustrate the slowdown in growth. This time around, the last system on the Top 500 list once was rated as the 384th quickest system in November 2013. “This represents the cheapest turnover level in the list in 2 decades,” the very best 500 announcement said. Basically, fewer new devices are joining the very best 500: In June 2013, the 500th program had fallen completely from #322.</p><p>Between 1994 and 2008, the last system at the top 500 list grew performance by an average of 90 percent every year. Since that time, the #500 program has improved its performance just 55 percent each year.</p><p>The combined performance of all 500 systems hit 274 petaflop/s in the most recent list, up from 250 petaflop/s six months ago and 223 petaflop/s twelve months ago. “This increase in installed performance as well exhibits a obvious slowdown in growth compared to the previous long-term trend,” the announcement said.</p><p>Supercomputer makers have been boosting speed partly by using co-processors as “accelerators” to handle a few of the work that could otherwise be achieved by CPUs. Sixty-two of the Top 500 systems have co-processors, with 44 of these using Nvidia’s graphics processing units. Nvidia is wishing to make ARM a major area of the supercomputing world by pairing 64-tad ARM server processors using its GPU accelerators.</p><p>Vendors could boost supercomputer performance much more by just creating ever bigger systems-but it wouldn’t necessarily get efficient. Companies want to figure out how to effectively create exascale supercomputers, which will be 1,000 times faster than a petaflop per second. Intel today declared brand-new Xeon Phi processors and a far more useful and lower-latency interconnect, calling the brand new architecture “the first practical step towards exascale.”</p><p>2 years ago, Intel said 40 to 50 gigaflops of performance per watt is required to hit an exaflop, but that milestone is regarded as at least many years away. The most effective Best 500 supercomputer, which reaches the Tokyo Institute of Technology and uses both Intel CPUs and Nvidia GPUs, can hit 4.5 gigaflops per watt.</p><h2 id="Japan-plans-130-petaflops-China-beating-number-crunching-supercomputer"><a href="#Japan-plans-130-petaflops-China-beating-number-crunching-supercomputer" class="headerlink" title="Japan plans 130-petaflops China-beating number-crunching supercomputer"></a>Japan plans 130-petaflops China-beating number-crunching supercomputer</h2><p>Sadly, name of supercomputing monster is the boring “AI Bridging Cloud Infrastructure.”</p><p>Japan is reportedly planning to build a 130-petaflops supercomputer costing $173 million (�131 million) that is thanks for completion next season.</p><p>Satoshi Sekiguchi, a director-basic at Japan’s ?National Institute of Advanced Industrial Science and Technology, where in fact the computer will be built, told Reuters: “As far as we know, there is little or nothing out there that is as fast.”</p><p>Based on the Top 500 blog listing the world’s fastest computers, the existing number-crunching champ is China’s 93-petaflops Sunway TaihuLight, followed by its Tianhe-2, to arrive in 34 petaflops. Japan’s most powerful system right now is normally a 13.5 petaflops model. Overall, Japan gets the fourth-largest quantity of supercomputers in the most notable 500 listing, following the US, China, and Germany.</p><p>The UK comes in sixth; the most effective system in the country is normally housed at the Met Workplace, and includes a max performance of 6.8 petaflops.</p><p>Like 498 out of the top 500 devices, Japan’s 27 supercomputers in the most notable 500 list all work Linux, and it is highly likely the brand new system will conduct so as well. It is not yet known who’ll construct the machine for the Japanese government-bidding for the task is open up until December 8.</p><p>Japan’s new machine will be used in neuro-scientific Artificial Intelligence, which explains its rather boring brand: “AI Bridging Cloud Infrastructure,” or perhaps ABCI. Sekiguchi told Reuters that the machine will also be employed to “tap medical data to develop new products and services and applications.”</p><p>Apparently the program is to permit Japan’s corporations to book time in the supercomputer for a fee, so freeing them from the necessity to use US companies like Google and Microsoft.</p><p>The investment in the large system is part of a wider proceed to boost Japan’s standing in the wonderful world of technology. Recently, it’s been rather overshadowed by advancements in South Korea and China.</p><p>Even though Japan hopes to leap to the most notable of the supercomputer league desk with the brand new ABCI, China is doubtless constructing better machines that may but deprive Japan of this honour.</p>]]></content>
      
      
      <categories>
          
          <category> supercomputer </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Today Super Computer</title>
      <link href="/today-super-computer/"/>
      <url>/today-super-computer/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/1.jpg" alt=""></p><p>At any moment, there are many well-publicized supercomputers that operate at extremely high speeds relative to all other computers. The word is also sometimes applied to far slower (but still impressively fast) computer systems. The largest, most strong supercomputers are actually multiple PCs that perform parallel digesting. In general, there will be two parallel processing methods: symmetric multiprocessing (SMP) and massively parallel processing (MPP).</p><p>By June 2016, the most effective supercomputer in the world was the Sunway TaihuLight, in the city of Wixu found in China. A few figures on TaihuLight:</p><ul><li>40,960 64-bit, RISC processors with 260 cores each.</li><li>Peak performance of 125 petaflops (quadrillion floating point procedures per second).</li><li>32GB DDR3 memory space per compute node,  1.3 PB memory in total.</li><li>Linux-based Sunway Raise operating system (OS).</li></ul><a id="more"></a><p>Significant supercomputers throughout history:</p><ul><li>The first commercially successful supercomputer, the CDC (Control Data Corporation) 6600 was created by Seymour Cray. Released in 1964, the CDC 6600 had an individual CPU and cost $8 million - the equivalent of $60 million today. The CDC could handle three million floating level operations per second (flops).</li></ul><p>Cray continued to found a good supercomputer firm under his name found in 1972. Although the business has changed hands several times it really is still functioning. In September 2008, Cray and Microsoft introduced CX1, a $25,000 personal supercomputer targeted at markets such as  aerospace, automotive, academic, personal services and lifestyle sciences.</p><p>IBM is a keen competitor. The business’s Roadrunner, after the top-rated supercomputer, was twice as quickly as IBM’s Blue Gene and six circumstances as fast as any of other supercomputers in those days. IBM’s Watson is famous for having adopted cognitive computing to defeat champion Ken Jennings on Jeopardy!, a favourite quiz show.</p><p>In America, some supercomputer centers are interconnected on an Internet backbone known as vBNS or NSFNet. This network is the foundation for an evolving network infrastructure known as the National Technology Grid. Internet2 is a university-led task that is part of the initiative.</p><p>At the low end of supercomputing, clustering takes even more of a build-it-yourself method of supercomputing. The Beowulf Task offers guidance on how to put along a number of off-the-shelf pc processors, using Linux operating systems, and interconnecting the processors with Fast Ethernet. Applications must be written to control the parallel processing.</p><h2 id="10-million-core-supercomputer-hits-93-petaflop-s-tripling-speed-record"><a href="#10-million-core-supercomputer-hits-93-petaflop-s-tripling-speed-record" class="headerlink" title="10 million-core supercomputer hits 93 petaflop/s, tripling speed record"></a>10 million-core supercomputer hits 93 petaflop/s, tripling speed record</h2><p>There’s a fresh world’s fastest supercomputer for the first time in three years.</p><p>A Chinese supercomputer named Sunway TaihuLight today ranks as the world’s fastest, nearly tripling the prior supercomputer quickness record with a score of 93 petaflops per second. That’s 93 quadrillion floating point functions per second (or 93 million billion).</p><p>Sunway TaihuLight surpassed another Chinese supercomputer, Tianhe-2, which had been the world’s fastest for 3 consecutive years with speeds of 33.9 petaflop/s, based on the latest Top500.org rank released today. Top500 rankings derive from the Linpack benchmark, which necessitates each cluster “to resolve a dense program of linear equations.”</p><p>“Sunway TaihuLight, with 10,649,600 processing cores comprising 40,960 nodes, is doubly fast and 3 x as reliable as Tianhe-2,” the Top500 announcement explained. Sunway TaihuLight is probably the world’s most efficient systems, with “peak power consumption under load (jogging the HPL benchmark)… at 15.37MW, or 6 Gflops/Watt.”</p><p>The machine has memory of just one 1.3PB, or 32GB for every node. This is really not much memory considering just how many cores the machine has; if it employed “a far more reasonable amount of memory because of its size,” Sunway TaihuLight will be far more power-hungry. Sunway provides more than three times as many cores as Tianhe-2, but it uses less overall vitality-15.37MW vs 17.8MW.</p><p>Tianhe-2 have been the world’s fastest in six consecutive rankings, which happen to be released twice a yr. Tianhe-2 took the most notable spot for the 1st time in June 2013, defeating the previous most effective supercomputer, which kept the position just once.</p><p>Developed by China’s National Research Center of Parallel Pc Engineering &amp; Technology, Sunway TaihuLight is installed in the National Supercomputing Centre in Wuxi, China. While Tianhe-2 uses Intel processors, Sunway TaihuLight was created entirely with processors designed and created in China. Sunway TaihuLight runs on the custom interconnect based on PCIe 3.0 technology.</p><p>Each node in Sunway TaihuLight has one SW26010 chip, a new version of the ShenWei processor that makes speeds of 3 teraflop/s with 260 cores. This is a 1.45GHz 64-bit RISC processor, however the Leading500 announcement said “its underlying architecture is somewhat of a mystery.”</p><p>“In 3 teraflops, the brand new ShenWei silicon is on par with Intel’s ‘Knights Landing’ Xeon Phi, another many-core design, but one with a more public history,” Best500 stated. “In somewhat of related irony, it had been the united states embargo of high-end processors, including the Xeon Phi, imposed on several Chinese supercomputing centers in April 2015, which precipitated a far more concerted effort in that country to build up and manufacture such chips domestically. The embargo probably didn’t affect the TaihuLight time-line, because it had been set to receive the brand new ShenWei parts. Nonetheless it was extensively imagined that Tianhe-2 was in series to get an upgrade using Xeon Phi processors, which could have very likely raised its functionality into 100-petaflop territory prior to the Wuxi program came online.”</p><p>While the US embargo is set up because of concern about nuclear research, Top500.org said Sunway TaihuLight is planned for work with in exploration and engineering work in fields including climate, weather and Earth devices modelling, life science exploration, advanced manufacturing, and data analytics.</p>]]></content>
      
      
      <categories>
          
          <category> supercomputer </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>16 Petaflops and More Core Without It Blowing Up</title>
      <link href="/16-petaflops-and-more-core-without-it-blowing-up/"/>
      <url>/16-petaflops-and-more-core-without-it-blowing-up/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/4.jpg" alt=""></p><h2 id="With-16-petaflops-and-1-6M-cores-DOE-supercomputer-is-world’s-fastest"><a href="#With-16-petaflops-and-1-6M-cores-DOE-supercomputer-is-world’s-fastest" class="headerlink" title="With 16 petaflops and 1.6M cores, DOE supercomputer is world’s fastest"></a>With 16 petaflops and 1.6M cores, DOE supercomputer is world’s fastest</h2><p>Every six months, Earth’s most important supercomputers have a huge race to see that may lay claim to getting the world’s most effective high-performance computing cluster.</p><p>In the most recent Top 500 Supercomputer Sites list unveiled Monday morning hours, a newly assembled cluster constructed with IBM hardware at the Department of Energy’s Lawrence Livermore National Laboratory (LLNL) takes the most notable prize. Its speed? An impressive 16.32 petaflops, or 16 thousand trillion calculations per second. With 96 racks, 98,304 compute nodes, 1.6 million cores, and 1.6 petabytes of memory across 4,500 square feet, the IBM Blue Gene/Q system installed at LLNL overtakes the 10-petaflop, 705,000-core computer in Japan’s RIKEN Advanced Institute for Computational Research.</p><a id="more"></a><p>The Japanese computer have been world’s quickest twice in a row. Before that, the most notable spot happened by a Chinese program. The DOE pc, named “Sequoia”, was sent to LLNL between January and April. It is the first US system to be ranked #1 since November 2009.</p><p>To access 16 petaflops, Sequoia ran the Linpack benchmark for 23 hours with out a single main failing, LLNL division head Kim Cupps told Ars Friday in advance of the list’s release. The system is capable of hitting a lot more than 20 petaflops-during the checks it ran at 81 percent proficiency.</p><p>For a equipment with 1.6 million cores to perform for over 23 hours six weeks after the keep going rack arrived on our floor is nothing short of astounding, she said.</p><p>The cluster is incredibly efficient for one so large, with 7,890 kilowatts of power, in comparison to 12,659 kilowatts for the second-best K Computer. It’s generally cooled by drinking water running through small copper pipes encircling the node cards. Each cards retains 32 chips, with each chip having 16 cores.</p><p>The entire cluster is Linux-based. Compute Node Linux is run on almost 98,000 nodes, and Red Hat Business Linux runs on 768 I/O nodes which hook up to the file program, Cupps said.</p><p>To start out, the cluster is about a comparatively open network, allowing various scientists to use it. But after IBM’s debugging method is over around February 2013, the cluster will be transferred to a categorized network that isn’t available to academics or outside corporations. At that point, it’ll be devoted almost specifically to simulations targeted at extending the lifespan of nuclear weapons.</p><p>The sort of science we must carry out is lifetime extension programs for nuclear weapons, Cupps said. That will require suites of codes jogging. What we’re in a position to do upon this machine is to perform many calculations simultaneously on the device. You can turn various knobs in a brief time frame.</p><p>Blue Gene/Q runs on the PowerPC architecture that includes components support for transactional memory space, allowing more comprehensive real-world assessment of technology.</p><p>In November 2011’s Top 500 list, three of the top five clusters employed NVIDIA GPUs (graphics processing units) in combo with CPUs to accomplish very high speeds. This time around, just one of the very best five integrates GPUs, although the entire number in the most notable 500 integrating GPUs or identical accelerators rose from 39 to 58.</p><p>The use of GPUs in supercomputing tends to be experimental up to now, said Dave Turek, IBM vice president of high performance computing. The aim of this is to accomplish real technology, he said. GPUs are a bit more difficult to method for, he said.</p><p>While the majority of Top 500 computers use Ethernet or Infiniband as their primary interconnects, Sequoia uses IBM’s proprietary 5D Torus. It’s an optical network that delivers 40 Gbps throughput to IBM’s Blue Gene/Q clusters. I/O nodes are linked to the file program via Infiniband and the administration network uses Ethernet, Cupps stated.</p><p>IBM leads the most notable 500 list with 213 systems, before HP’s 138. Practically 80 percent-372 of the 500 systems-use Intel processors, accompanied by 63 employing AMD Operton and 58 using IBM Power.</p><p>Three DOE systems happen to be in the very best 10. The others hail from Japan, Germany, China, Italy, and France. All 10 have effectiveness of at least at least 1.27 petaflops.</p><p>Petascale computers have become relatively commonplace because the IBM Roadrunner program in Los Alamos National Laboratory was first the first to hit a petaflop in 2008. Actually, each of the top 20 devices on the brand new list hit at least a petaflop. Exascale, which would be 1,000 times faster, is the next big breakthrough for the IBMs, HPs, and Crays of the universe to aspire to.</p><p>But a big progress in price-performance is necessary. Today’s technology could level up a lot higher-it merely wouldn’t be practical. Supercomputers are naturally pricey (a lot more expensive than the latest MacBook Pro). The K Computer in Japan, for instance, cost a lot more than $1 billion to build and $10 million to operate each year. Livermore informed us it spent approximately $250 million on Sequoia.</p><p>We’re able to get another order of magnitude with this technology if somebody would compose a check, Turek stated. But no one would want to write that check.�</p><h3 id="How-to-use-a-good-million-core-supercomputer-without-it-blowing-up-in-that-person"><a href="#How-to-use-a-good-million-core-supercomputer-without-it-blowing-up-in-that-person" class="headerlink" title="How to use a good million-core supercomputer-without it blowing up in that person"></a>How to use a good million-core supercomputer-without it blowing up in that person</h3><p><em>Throwing more cores by science: risky for high reward.</em></p><p>At the Lawrence Livermore National Laboratory in California, a supercomputer named “Sequoia” puts almost every other computer on earth to shame. With 1.6 million processor cores (16 per CPU) across 96 racks, Sequoia can perform 16 thousand trillion calculations per second, or 16.32 petaflops.</p><p>Who would demand such horsepower? The IBM Blue Gene/Q-based program was designed for the Section of Strength for simulations designed to lengthen the lifespan of nuclear weapons. But also for a limited time, the device is being distributed around outside researchers to execute a variety of tests, a couple of hours at a time.</p><p>One of the first to have good thing about this opportunity was Stanford University’s Center for Turbulence Research-and it wasn’t hesitant about seeing what this equipment is really capable of. For three hours on Tuesday of last week, researchers from the center remotely logged in to Sequoia to perform a computational liquid dynamics (CFD) simulation on a million cores at once-1,048,576 cores, to be exact.</p><p>It’s part of a good project to test noise made by supersonic jet engines and support design engines that happen to be a bit quieter. The work is sponsored partly by the united states Navy, which is concerned about “hearing damage that sailors on aircraft carrier decks come across as a result of jet noise,” Study Associate Joseph Nichols of the guts for Turbulence Exploration told Ars.</p><p>Using huge supercomputers to solve complex scientific problems is usually in no way unique nowadays. Larger amounts of cores don’t necessarily translate to the most effective speed, either, as a result of dissimilarities between processors and the styles of supercomputers. The million-core run is normally intriguing, but it additionally poses extreme issues in trying to work with all those cores simultaneously without things going incorrect.</p><p>Believe it or not, three several hours with a million cores wasn’t enough to generate a real dent in the plane noise project. Despite planning work aimed at reducing out bottlenecks, it was just enough time to make certain the code ran properly also to get yourself a sense of the possibilities that million-core PCs can offer.</p><p>“This is really showing what we are able to do later on,” Nichols said. “The simulations take some time to boot up and go through initialization. We performed tune the I/O for the Blue Gene architecture, but it is nonetheless slower than the blindingly quickly computation and interaction speeds. Depending on how much info gets written, the I/O can truly add an extra chunk of period to the overhead.”</p><p>The problems are all over the area, including “How can you write into one file from a million processors that are trying to step on one another?” Nichols said. “That was an interesting thing. It kind of depended on the interconnect, too. Only a number of the processors happen to be linked to the disk, and that means you need to rearrange data to achieve the right performance.”</p><h2 id="Extra-cores-or-better-cores"><a href="#Extra-cores-or-better-cores" class="headerlink" title="Extra cores, or better cores?"></a>Extra cores, or better cores?</h2><p>Sequoia was named the world’s quickest supercomputer in June 2012. Sequoia soon after fell to second place behind a 17.59-petaflop system at the Oak Ridge Countrywide Laboratory, but it is still the only system at the top 500 supercomputers list with a million or even more cores.</p><p>Of course, choosing “more cores” isn’t necessarily the easiest method to tackle a supercomputing problem. That Oak Ridge pc, named Titan, hit 17.59 petaflops using “only” 560,640 cores. The Titan program was developed by the supercomputer producer Cray, and it uses Nvidia design processing units furthermore to classic CPUs to get dramatic increases in quickness.</p><p>There are pros and cons to different approaches. Dave Turek, IBM vice president of powerful computing, told Ars this past year that GPUs happen to be more difficult to program for, and the GPU-significantly less Sequoia is for “real science.” Titan, it should be noted, does plenty of real technology, tackling problems linked to climate transformation, astrophysics, and more. With both CPUs and GPUs in something, Titan’s CPUs guide the simulations but hand off do the job to the GPUs, which can handle a lot more calculations at once despite using only slightly more electricity.</p><p>Throwing more cores at a problem doesn’t invariably result in performance gains. Code needs to be carefully well prepared to take into account the bottlenecks that come up when information is transmitted from one core to another (unless the request is so parallel that each core could work on split calculations without ever talking to each other).</p><p>Previously, Nichols’ biggest calculation was performed for approximately 100 hours on 131,072 cores on a Blue Gene/P system. The same calculation could possibly be done on a million Sequoia cores in about 8 to 12 time, he said.</p><p>Besides accelerating lengthy calculations, even more cores and faster supercomputers will let scientists tackle a lot more complicated problems.</p><p>“Having more cores, either you reduce the time to choice or you solve more technical problems, problems involving circulation that involve, say, chemical substance reactions, combustion,” stated Parviz Moin, director of the guts for Turbulence Research. “These are grand challenge problems that involve many, many more equations and lots of work that will be distributed among the processors.”</p><h2 id="Getting-code-geared-up-for-the-million-core-run"><a href="#Getting-code-geared-up-for-the-million-core-run" class="headerlink" title="Getting code geared up for the million-core run"></a>Getting code geared up for the million-core run</h2><p>Nichols and team use the Navier-Stokes equations and codenamed CharLES for the plane noise simulations (LES means large eddy simulation). They’re employing the same code for different projects to research scram jets (supersonic combustion ramjets), which could travel at 10 situations the speed of audio.</p><p>We’ve written before about large supercomputing runs-for example, one using 50,000 cores on the Amazon Elastic Compute Cloud. That one was “embarrassingly parallel,” where the calculations are all independent of each other. Which means the swiftness of the interconnect, the connections between each processing key, didn’t really matter.</p><p>The million-core run was not only substantially bigger, it was more complicated. “It is parallel but there is connection [between processors] included,” Moin explained. “Each core is not independent.”</p><p>As we wrote in August 2011, Blue Gene/Q uses “transactional memory space” to solve many of the issues that help to make highly scalable parallel development so hard. But prep function by humans is still required.</p><p>Nichols caused the Lawrence Livermore individuals to optimize the code for Sequoia, avoiding slowdowns found in I/O efficiency and minimizing the communication needed in each step.</p><p>Sequoia uses IBM’s proprietary 5-dimensional Torus interconnect. Nichols explains:</p><p>“Each compute node is linked to 10 of its nearest neighbours. There will be five measurements and it goes forwards and backward along each dimension. That’s 5x2 and you get 10 connections with these optical links.”</p><p>“You can communicate with processors that are even more away, but it includes a bigger latency. Latency for the nearest neighbour is normally 80 nanoseconds, which is very amazing. The calculation uses the interconnect so that communication overhead was very little actually at a million processors.”</p><p>Performance scaled almost in a one-to-1 ratio with the increase in cores, with 83 percent efficiency. Nichols clarifies that since heading from 131,000 cores to at least one 1 million multiplies the amount of cores by 8, one would want a speed-up of 8 as well.</p><p>The true speed-up was 6.6, which is “83 percent of the perfect speed-up we would like to see. It signifies that as we’re adding considerably more cores the code gets faster and faster, also at the million level. This is amazing for a CFD simulation, because in CFD simulations each one of these sub-domains features to talk to its neighbours at each time step to talk about wave information.”</p><p>Modelling plane engines is sophisticated, as Stanford notes in a description of the project: “These complex simulations let researchers to peer inside and measure processes happening within the tough exhaust environment that’s otherwise inaccessible to experimental devices.”</p><p>The simulations allow Stanford to test how adjustments to the engine nozzle and chevrons impact noise. With supercomputers these simulations can be carried out without building physical designs or tests in wind tunnels.</p><p>“That’s a really complicated difficulty because you possess shock waves found in engines which are incredibly thin scales when compared to amount of the combustion,” Nichols said. “And then you have combustion as well as turbulence in everything. The theory is that people can predict the behaviour. If given enough resolution we can predict what will happen with various kinds of designs.”</p><p>The analysis of scram jets and the conditions under which they might fail involves similar complexity. “NASA is very much enthusiastic about such vehicles for access to space,” Moin stated. “These will be air-breathing vehicles instead of rockets. They take their personal oxygen to orbit; they’re heavier due to that. They have to carry liquid oxygen.”</p><p>For now, the researchers must continue this work with paltry sub-million-main supercomputers. But most likely it won’t be a long time before supercomputers as powerful as Sequoia will be the standard for such research.</p><p>As a high school student in 1994, Nichols attended a summer season course at Lawrence Livermore and done the Cray Y-MP. At that time, it was among the faster machines on earth.</p><p>“Now Sequoia is 10 million times stronger than that equipment,” Nichols said. “This is presenting us a glimpse into the future, that it is really possible to run on a million cores.”</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
